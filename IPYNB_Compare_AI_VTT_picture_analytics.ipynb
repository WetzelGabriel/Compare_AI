{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoProcessor, AutoModelForImageTextToText, AutoTokenizer, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model ID and the local path where the model should be stored\n",
    "model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n",
    "local_model_path = os.path.expanduser(\"~/myProject_LLM/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(0)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What are these?\n",
      "assistant\n",
      "These are two cats, one on the left and one on the right. They are lying on a pink blanket, which is placed on a couch. The cat on the left is sleeping, while the one on the right is resting.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the image you want to describe\n",
    "image_path = os.path.expanduser(\"~/myProject_LLM/myDocs/picture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image from the URL or local path with error handling\n",
    "if os.path.exists(image_path):  # Check if it's a local file\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image.verify()  # Verify that the file is an image\n",
    "        image = Image.open(image_path)  # Reopen the image after verification\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        image = None\n",
    "else:  # Assume it's a URL\n",
    "    try:\n",
    "        response = requests.get(image_path, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                image = Image.open(response.raw)\n",
    "                image.verify()  # Verify that the file is an image\n",
    "                image = Image.open(response.raw)  # Reopen the image after verification\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {e}\")\n",
    "                image = None\n",
    "        else:\n",
    "            print(f\"Failed to fetch image. HTTP status code: {response.status_code}\")\n",
    "            image = None\n",
    "    except requests.exceptions.MissingSchema:\n",
    "        print(f\"Invalid URL: {image_path}. Please provide a valid URL or local file path.\")\n",
    "        image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What are these?\n",
      "assistant\n",
      "These are three young tigers, also known as cubs, lying down on the ground. They are in a natural setting, possibly a forest or a savanna, with a dirt path in the background. The cubs are close to each other, and they appear to be enjoying each other's company.\n"
     ]
    }
   ],
   "source": [
    "if image is None:\n",
    "    print(\"Image is not loaded properly.\")\n",
    "else:\n",
    "    try:\n",
    "        inputs = processor(images=image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "        output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "        print(processor.decode(output[0][2:], skip_special_tokens=True))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model inference: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the image you want to describe\n",
    "video_path = os.path.expanduser(\"~/myProject_LLM/myDocs/BoschSmartCameras_11-04-2025_21-05-26-315.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a video\n",
    "def infer_video(model, video_path, frame_skip=10):\n",
    "    # Öffnet das Video, das sich unter dem Pfad video_path befindet, mit OpenCV.\n",
    "    # cap ist ein Objekt, das Zugriff auf die Frames des Videos ermöglicht.\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    results = []\n",
    "    frames = []  # Liste zum Sammeln von Frames\n",
    "\n",
    "    for i in range(frame_count):\n",
    "        # cap.read() liest das nächste Frame aus dem Video.\n",
    "        # ret: Gibt True zurück, wenn das Frame erfolgreich gelesen wurde, sonst False.\n",
    "        # frame: Das gelesene Frame als NumPy-Array.\n",
    "        # Wenn kein Frame mehr gelesen werden kann (ret == False), wird die Schleife abgebrochen.\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Konvertiert das Frame von OpenCVs BGR-Farbformat in das RGB-Format.\n",
    "        # Wandelt das NumPy-Array in ein PIL-Bild um, das für die Verarbeitung durch das Modell geeignet ist.\n",
    "        if i % frame_skip == 0:\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frames.append(frame)\n",
    "            print(f\"Selected {len(frames)} frames for processing.\")\n",
    "            inputs = processor(images=frame, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "            output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "            results.append(processor.decode(output[0][2:], skip_special_tokens=True))\n",
    "\n",
    "    cap.release()  # Gibt das Video-Objekt frei, um Ressourcen freizugeben.\n",
    "    # Gibt die Liste der Ergebnisse zurück, die während der Verarbeitung jedes Frames gesammelt wurden.\n",
    "    return frames, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 frames for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 7 frames for processing.\n"
     ]
    }
   ],
   "source": [
    "video_frames, video_outputs  = infer_video(model, video_path, frame_skip=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically patio furniture. They include a table with chairs, a bench, and a chair. The table is placed on a patio area with a brick floor, and the chairs are arranged around it. The bench is positioned next to the table, and the chair is placed on the opposite side of the table. The furniture appears to be designed for relaxation and socializing, suitable for a patio or garden setting.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 2:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically patio furniture. They include a table with chairs, a bench, and a chair. The table is placed on a patio area with a brick floor, and the chairs are arranged around it. The bench is positioned next to the table, and the chair is placed on the opposite side of the table. The furniture appears to be designed for relaxation and socializing, suitable for a patio or garden setting.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 3:\n",
      "What are these?\n",
      "assistant\n",
      "These are a series of images taken from a camera mounted on a tripod, capturing a scene in a backyard. The setting includes a white building with red shutters, a paved area with a table and chairs, and a garden with various plants and trees. There is also a person standing near the entrance of the building, and a person riding a scooter is visible in the foreground. The image quality is low, with some blurriness and noise, which suggests it may have been taken with a lower-quality camera or in poor lighting conditions.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 4:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically a table and a chair. They are placed on a patio or garden area, which is surrounded by a paved surface and a garden with plants and trees.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 5:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically a table and a chair. They are placed on a patio or garden area, which is surrounded by a paved surface and a grassy area. The table is positioned to the left of the chair, and the chair is to the right of the table.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 6:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically a table and a chair. They are placed on a patio or garden area, which is surrounded by a paved surface and a garden with plants and trees.\n",
      "--------------------------------------------------------------------------------\n",
      "Output 7:\n",
      "What are these?\n",
      "assistant\n",
      "These are outdoor furniture items, specifically a table and a chair. The table is placed on a patio or garden area, while the chair is situated near the entrance of a building.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(video_outputs):\n",
    "    print(f\"Output {i + 1}:\\n{output.strip()}\\n{'-' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = '/home/gabriel/myProject/myvenv/.env'\n",
    "\n",
    "def ask_question(question, video_outputs):\n",
    "    from langchain.schema import Document\n",
    "    try:\n",
    "        # Load environment variables from .env file\n",
    "        load_dotenv(dotenv_path)\n",
    "\n",
    "        # Combine the video_outputs into a single string\n",
    "        video_outputs_text = \"\\n\\n\".join([f\"Output {i + 1}:\\n{output.strip()}\" for i, output in enumerate(video_outputs)])\n",
    "\n",
    "        # Create a list of Documents for input_documents\n",
    "        input_documents = [Document(page_content=video_outputs_text)]\n",
    "\n",
    "        # Access the API key\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key is missing.\")\n",
    "\n",
    "        # Initialize the LLM and QA chain\n",
    "        llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "        chain = load_qa_chain(llm=llm, chain_type='stuff')\n",
    "\n",
    "        # Run the chain with the required inputs\n",
    "        response = chain.run(input_documents=input_documents, question=question)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ask_question: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Summarize the describtion of the video with following\n",
    "text description of the frames of the video.\n",
    "Please be precise and concise.\n",
    "No repetition of the same information\n",
    "if they are visible on different frames.\n",
    "Formulate it in manner that is great to read\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26451/4204890620.py:26: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
      "/tmp/ipykernel_26451/4204890620.py:27: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm=llm, chain_type='stuff')\n",
      "/tmp/ipykernel_26451/4204890620.py:30: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input_documents=input_documents, question=question)\n"
     ]
    }
   ],
   "source": [
    "video_description = ask_question(question, video_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The video shows a series of images taken from a camera mounted on a tripod in a backyard setting. The scene includes a white building with red shutters, a paved area with a table and chairs, and a garden with plants and trees. There are also people visible in the frames, including one standing near the building and another riding a scooter. The image quality is low, suggesting it may have been taken with a lower-quality camera or in poor lighting conditions.\n"
     ]
    }
   ],
   "source": [
    "print(video_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
