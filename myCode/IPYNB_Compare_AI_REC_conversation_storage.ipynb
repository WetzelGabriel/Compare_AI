{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook for capturing a discussion (or possibily own thoughts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "# import torch\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "import LIB_AudioLib as myALib\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio if needed\n",
    "nest_asyncio.apply()\n",
    "import nest_asyncio\n",
    "\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Speicher freigeben\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"medium\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Index: 0, Device Name: HDA Intel PCH: ALC1150 Analog (hw:0,0)\n",
      "Device Index: 1, Device Name: HDA Intel PCH: ALC1150 Digital (hw:0,1)\n",
      "Device Index: 3, Device Name: Plantronics Blackwire 3220 Seri: USB Audio (hw:1,0)\n",
      "Device Index: 4, Device Name: HDA NVidia: HDMI 0 (hw:2,3)\n",
      "Device Index: 5, Device Name: HDA NVidia: HDMI 1 (hw:2,7)\n",
      "Device Index: 6, Device Name: HDA NVidia: HDMI 2 (hw:2,8)\n",
      "Device Index: 7, Device Name: HDA NVidia: HDMI 3 (hw:2,9)\n",
      "Device Index: 8, Device Name: Jabra SPEAK 510 USB: Audio (hw:3,0)\n",
      "Device Index: 9, Device Name: Dell Universal Dock D6000: USB Audio (hw:4,0)\n",
      "Device Index: 11, Device Name: sysdefault\n",
      "Device Index: 12, Device Name: front\n",
      "Device Index: 13, Device Name: surround21\n",
      "Device Index: 14, Device Name: surround40\n",
      "Device Index: 15, Device Name: surround41\n",
      "Device Index: 16, Device Name: surround50\n",
      "Device Index: 17, Device Name: surround51\n",
      "Device Index: 18, Device Name: surround71\n",
      "Device Index: 19, Device Name: iec958\n",
      "Device Index: 20, Device Name: spdif\n",
      "Device Index: 21, Device Name: pipewire\n",
      "Device Index: 22, Device Name: dmix\n",
      "Device Index: 23, Device Name: default\n"
     ]
    }
   ],
   "source": [
    "output_devices, default_device_index = myALib.get_audio_output_devices()\n",
    "\n",
    "for device in output_devices:\n",
    "    print(f\"Device Index: {device['index']}, Device Name: {device['name']}\")\n",
    "\n",
    "# Format the options for the radio button\n",
    "options = [f\"{device['name']} (Index: {device['index']})\" for device in output_devices]\n",
    "\n",
    "# Determine the default index for the radio button\n",
    "if default_device_index is not None:\n",
    "    default_index = next((i for i, d in enumerate(output_devices) if d['index'] == default_device_index), 0)\n",
    "else:\n",
    "    default_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Generate a sound on device for 1 second\n",
    "selected_device_index = 8  # Replace with the index of the device you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Silence detected.\n",
      "Speech length: 88.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/myProject_LLM/myvenv_llm/lib/python3.12/site-packages/whisper/transcribe.py:130: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/gabriel/myProject_LLM/myvenv_llm/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jetzt möchte ich die Kamerastrategie beschreiben. Also wir haben drei unterschiedliche Software Module. Die eine ist die App, das zweite ist die Firmware in der Kamera, die dritte ist die Cloud Software. In der Firmware für die Kamera haben wir zwei Teile, das Betriebssystem und die Home spezifische Lösung. In der Cloud Software haben wir zwei Module, eine ist den Residential Cloud, der behandelt alle Funktionen die wir in dem spezifisch für die Home Kamera haben und den BT Cloud, das ist der Video Relay und der Video Relay beinhaltet eigentlich die Aufnahme der Clips, die Speicherung der Clips in einem Speicher und die Indexierung mit weiterer Information wie Daten und so weiter.\n",
      "Recording...\n",
      "Silence detected.\n",
      "Speech length: 0.21 seconds\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "text_filename=f\"text_{timestamp}.txt\"\n",
    "speech_length = 1\n",
    "\n",
    "while speech_length > 0.5:\n",
    "    # Generate a sound on the selected device for 1 second\n",
    "    myALib.generate_sound_on_device(device_index=selected_device_index, duration=1)\n",
    "\n",
    "    # Capture question from the microphone for 10 seconds and save it to \"input.wav\"\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    speech_length = myALib.capture_speech_from_microphone(duration=0, output_filename=f\"input_{timestamp}.wav\")\n",
    "    print(f\"Speech length: {speech_length:.2f} seconds\")\n",
    "\n",
    "    if speech_length > 0.5:\n",
    "        result = model.transcribe(f\"input_{timestamp}.wav\")\n",
    "\n",
    "        # Save the transcription to a text file\n",
    "        with open(text_filename, \"a\") as text_file:\n",
    "            text_file.write(f\"{result[\"text\"]}\\n\\n\")\n",
    "        # Print the transcription\n",
    "        print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # Modell löschen\n",
    "# torch.cuda.empty_cache()  # Speicher freigeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported sample rates for device 8: [8000, 16000, 48000]\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "def check_supported_sample_rates(device_index):\n",
    "    p = pyaudio.PyAudio()\n",
    "    supported_rates = []\n",
    "    for rate in [8000, 16000, 22050, 32000, 44100, 48000, 96000]:\n",
    "        try:\n",
    "            if p.is_format_supported(rate,\n",
    "                                     output_device=device_index,\n",
    "                                     output_channels=1,\n",
    "                                     output_format=pyaudio.paInt16):\n",
    "                supported_rates.append(rate)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    p.terminate()\n",
    "    return supported_rates\n",
    "\n",
    "# Check supported sample rates for device 8\n",
    "device_index = 8\n",
    "supported_rates = check_supported_sample_rates(device_index)\n",
    "print(f\"Supported sample rates for device {device_index}: {supported_rates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported sample rates for device 3: [8000, 16000, 32000, 44100, 48000]\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "def check_supported_sample_rates(device_index):\n",
    "    p = pyaudio.PyAudio()\n",
    "    supported_rates = []\n",
    "    for rate in [8000, 16000, 22050, 32000, 44100, 48000, 96000]:\n",
    "        try:\n",
    "            if p.is_format_supported(rate,\n",
    "                                     output_device=device_index,\n",
    "                                     output_channels=1,\n",
    "                                     output_format=pyaudio.paInt16):\n",
    "                supported_rates.append(rate)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    p.terminate()\n",
    "    return supported_rates\n",
    "\n",
    "# Check supported sample rates for device 8\n",
    "device_index = 3\n",
    "supported_rates = check_supported_sample_rates(device_index)\n",
    "print(f\"Supported sample rates for device {device_index}: {supported_rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prompt:\n",
    "\n",
    "Korrigier und Formatier bitte folgenden Speech-to-text Text in klaren Abschnitten mit Überschriften, Numerierung, Auflistung und Unterpunkten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
