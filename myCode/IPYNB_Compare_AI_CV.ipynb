{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time\n",
    "import wave\n",
    "import pyttsx3\n",
    "import subprocess\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = '/home/gabriel/myProject/myvenv/.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pa_audio_output_devices():\n",
    "    \"\"\"\n",
    "    This function lists all available audio output devices on the system.\n",
    "    \n",
    "    It uses the pyaudio library to query the available audio devices and \n",
    "    filters out the output devices. The function returns a list of dictionaries \n",
    "    containing the device name and its corresponding index.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries with 'name' and 'index' keys.\n",
    "    \"\"\"\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Initialize an empty list to store output devices\n",
    "    output_devices = []\n",
    "    \n",
    "    # Get the number of available devices\n",
    "    device_count = p.get_device_count()\n",
    "    \n",
    "    # Iterate over the devices and filter out the output devices\n",
    "    for idx in range(device_count):\n",
    "        device_info = p.get_device_info_by_index(idx)\n",
    "        if device_info['maxOutputChannels'] > 0:\n",
    "            output_devices.append({\n",
    "                'name': device_info['name'],\n",
    "                'index': idx\n",
    "            })\n",
    "    \n",
    "    # Terminate the PyAudio instance\n",
    "    p.terminate()\n",
    "    \n",
    "    return output_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sd_audio_output_devices():\n",
    "    \"\"\"\n",
    "    This function lists all available audio output devices on the system.\n",
    "    \n",
    "    It uses the sounddevice library to query the available audio devices and \n",
    "    filters out the output devices. The function returns a list of dictionaries \n",
    "    containing the device name and its corresponding index.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries with 'name' and 'index' keys.\n",
    "    \"\"\"\n",
    "    # Get the list of all available audio devices\n",
    "    devices = sd.query_devices()\n",
    "    \n",
    "    # Initialize an empty list to store output devices\n",
    "    output_devices = []\n",
    "    \n",
    "    # Iterate over the devices and filter out the output devices\n",
    "    for idx, device in enumerate(devices):\n",
    "        if device['max_output_channels'] > 0:\n",
    "            output_devices.append({\n",
    "                'name': device['name'],\n",
    "                'index': idx\n",
    "            })\n",
    "    \n",
    "    return output_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example usage\n",
    "output_devices = list_pa_audio_output_devices()\n",
    "for device in output_devices:\n",
    "    print(f\"Device Index: {device['index']}, Device Name: {device['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example usage\n",
    "output_devices = list_sd_audio_output_devices()\n",
    "for device in output_devices:\n",
    "    print(f\"Device Index: {device['index']}, Device Name: {device['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sample_rate_supported(device_index, sample_rate):\n",
    "    \"\"\"\n",
    "    Check if the given sample rate is supported by the specified device.\n",
    "    \n",
    "    Args:\n",
    "        device_index (int): The index of the audio output device.\n",
    "        sample_rate (int): The sample rate to check.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the sample rate is supported, False otherwise.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    try:\n",
    "        if device_index is None:\n",
    "            device_index = p.get_default_input_device_info()['index']\n",
    "        device_info = p.get_device_info_by_index(device_index)\n",
    "        supported = p.is_format_supported(sample_rate,\n",
    "                                          output_device=device_index,\n",
    "                                          output_channels=1,\n",
    "                                          output_format=pyaudio.paInt16)\n",
    "    except ValueError:\n",
    "        supported = False\n",
    "    p.terminate()\n",
    "    return supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sound_on_device(device_index, duration=1, frequency=440, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    This function generates a sound on a specified audio output device for a given duration.\n",
    "    \n",
    "    Args:\n",
    "        device_index (int): The index of the audio output device.\n",
    "        duration (float): The duration of the sound in seconds. Default is 1 second.\n",
    "        frequency (float): The frequency of the sound in Hz. Default is 440 Hz (A4 note).\n",
    "        sample_rate (int): The sample rate in Hz. Default is 44100 Hz.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not is_sample_rate_supported(device_index, sample_rate):\n",
    "        raise ValueError(f\"Sample rate {sample_rate} is not supported by device {device_index}\")\n",
    "    \n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Generate the sound wave\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    wave = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "    \n",
    "    # Convert the wave to 16-bit PCM format\n",
    "    wave = (wave * 32767).astype(np.int16)\n",
    "    \n",
    "    # Open the audio stream\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=sample_rate,\n",
    "                    output=True,\n",
    "                    output_device_index=device_index)\n",
    "    \n",
    "    # Play the sound\n",
    "    stream.write(wave.tobytes())\n",
    "    \n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    # Terminate the PyAudio instance\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_speech_from_microphone(duration=5, sample_rate=44100, channels=1, output_filename=\"output.wav\"):\n",
    "    \"\"\"\n",
    "    This function captures audio from the microphone for a specified duration and saves it to a WAV file.\n",
    "    \n",
    "    Args:\n",
    "        duration (float): The duration of the recording in seconds. Default is 5 seconds.\n",
    "        sample_rate (int): The sample rate in Hz. Default is 44100 Hz.\n",
    "        channels (int): The number of audio channels. Default is 1 (mono).\n",
    "        output_filename (str): The name of the output WAV file. Default is \"output.wav\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not is_sample_rate_supported(None, sample_rate):\n",
    "        raise ValueError(f\"Sample rate {sample_rate} is not supported by the default input device\")\n",
    "    \n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Open the audio stream for recording\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=channels,\n",
    "                    rate=sample_rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    \n",
    "    # Initialize an empty list to store the recorded frames\n",
    "    frames = []\n",
    "    \n",
    "    # Calculate the number of frames to record\n",
    "    num_frames = int(sample_rate / 1024 * duration)\n",
    "    \n",
    "    # Record the audio\n",
    "    for _ in range(num_frames):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    # Terminate the PyAudio instance\n",
    "    p.terminate()\n",
    "    \n",
    "    # Save the recorded frames to a WAV file\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_speech_on_device(file_path, device_index):\n",
    "    \"\"\"\n",
    "    This function plays a sound file on a specified audio output device.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the sound file (WAV format).\n",
    "        device_index (int): The index of the audio output device.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the sound file\n",
    "    wf = wave.open(file_path, 'rb')\n",
    "    \n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Check if the device supports the desired sample rate\n",
    "    try:\n",
    "        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                        channels=wf.getnchannels(),\n",
    "                        rate=wf.getframerate(),\n",
    "                        output=True,\n",
    "                        output_device_index=device_index)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Falling back to default sample rate.\")\n",
    "        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                        channels=wf.getnchannels(),\n",
    "                        rate=wf.getframerate(),\n",
    "                        output=True)\n",
    "    \n",
    "    \n",
    "    # Read data from the sound file\n",
    "    data = wf.readframes(1024)\n",
    "    \n",
    "    # Play the sound\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(1024)\n",
    "    \n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    # Terminate the PyAudio instance\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pico_text_to_speech(text, output_filename=\"tts_output.wav\", language='de-DE'):\n",
    "    \"\"\"\n",
    "    This function converts text to speech using Pico TTS and saves it to a WAV file.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to convert to speech.\n",
    "        output_filename (str): The name of the output WAV file. Default is \"tts_output.wav\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Use Pico TTS to convert text to speech and save it to a WAV file\n",
    "    command = ['pico2wave', '--wave', output_filename, \"-l\", language, text]\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"output.wav\")\n",
    "\n",
    "# Print the transcription\n",
    "print(\"Transcription:\", result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Convert text to speech and save it to \"tts_output.wav\"\n",
    "pico_text_to_speech(\"Hallo zusammen, ich freue mich über tolle Fortschritte\", output_filename=\"tts_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Play the TTS output file \"tts_output.wav\" on device 7\n",
    "play_speech_on_device(file_path=\"tts_output.wav\", device_index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example usage: Capture voice from the microphone for 15 seconds and save it to \"output.wav\"\n",
    "capture_speech_from_microphone(duration=15, output_filename=\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Play the speech file \"output.wav\" on device 3\n",
    "play_speech_on_device(file_path=\"output.wav\", device_index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"output.wav\")\n",
    "\n",
    "# Print the transcription\n",
    "print(\"Transcription:\", result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_text_to_speech(result[\"text\"], output_filename=\"tts2_output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Play the speech file \"output.wav\" on device 3\n",
    "play_speech_on_device(file_path=\"tts2_output.wav\", device_index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "async def upload_file(file_path):\n",
    "    \"\"\"\n",
    "    Upload a file as a data source for the LLM.\n",
    "    Args:\n",
    "        file_path (str): The path to the file to upload.\n",
    "    Returns:\n",
    "        LocalFileLoader: The loaded file.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = []\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(documents, question):\n",
    "    \"\"\"\n",
    "    Ask a question to the LLM using the uploaded documents.\n",
    "    Args:\n",
    "        documents (List[Document]): The list of documents to use as context.\n",
    "        question (str): The question to ask.\n",
    "    Returns:\n",
    "        str: The response from the LLM.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv(dotenv_path)\n",
    "\n",
    "    # Access the API key\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "    chain = load_qa_chain(llm=llm, chain_type='stuff')\n",
    "    response = chain.run(input_documents=documents, question=question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Print the response from the LLM.\n",
    "    Args:\n",
    "        response (str): The response to print.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/gabriel/myProject/myDocs/CV_Gabriel_250302_G.pdf\"\n",
    "documents = asyncio.run(upload_file(file_path))\n",
    "# Define the role or instructions for the OpenAI model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "question = \"\"\"\n",
    "You are a candidate for a job interview.\n",
    "Your resume is the file that is uploaded.\n",
    "In this file, you find everything about you,\n",
    "that you should know\n",
    "Your task is to respond to questions in German\n",
    "that the human ressources manager is asking you.\n",
    "The human resource manager is sitting in front of you.\n",
    "You only respond to the following question: Say hello to him in few nice words.\n",
    "\"\"\"\n",
    "response = ask_question(documents,question)\n",
    "print_response(response)\n",
    "pico_text_to_speech(response, output_filename=\"output1.wav\")\n",
    "# Play the response file \"output1.wav\" on device 3\n",
    "play_speech_on_device(file_path=\"output1.wav\", device_index=7)\n",
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "generate_sound_on_device(device_index=7, duration=1)\n",
    "\n",
    "# Capture question from the microphone for 15 seconds and save it to \"input2.wav\"\n",
    "capture_speech_from_microphone(duration=15, output_filename=\"input2.wav\")\n",
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"input2.wav\")\n",
    "# Print the transcription\n",
    "print(\"Transcription:\", result[\"text\"])\n",
    "\n",
    "question = f\"\"\"\n",
    "You are a candidate for a job interview.\n",
    "Your resume is the file that is uploaded.\n",
    "In this file, you find everything about you,\n",
    "that you should know\n",
    "Your task is to respond to questions in German\n",
    "that the human ressources manager is asking you.\n",
    "The human resource manager is sitting in front of you.\n",
    "You only respond to the following question:\n",
    "{result[\"text\"]}\n",
    "\"\"\"\n",
    "\n",
    "response = ask_question(documents,question)\n",
    "print_response(response)\n",
    "pico_text_to_speech(response, output_filename=\"output2.wav\")\n",
    "# Play the response file \"output2.wav\" on device 3\n",
    "play_speech_on_device(file_path=\"output2.wav\", device_index=7)\n",
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "generate_sound_on_device(device_index=7, duration=1)\n",
    "\n",
    "# Capture question from the microphone for 15 seconds and save it to \"input2.wav\"\n",
    "capture_speech_from_microphone(duration=15, output_filename=\"input3.wav\")\n",
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"input3.wav\")\n",
    "# Print the transcription\n",
    "print(\"Transcription:\", result[\"text\"])\n",
    "\n",
    "question = f\"\"\"\"\n",
    "You are a candidate for a job interview.\n",
    "Your resume is the file that is uploaded.\n",
    "In this file, you find everything about you,\n",
    "that you should know\n",
    "Your task is to respond to questions in German\n",
    "that the human ressources manager is asking you.\n",
    "The human resource manager is sitting in front of you.\n",
    "You only respond to the following question:\n",
    "{result[\"text\"]}\n",
    "\"\"\"\n",
    "response = ask_question(documents,question)\n",
    "print_response(response)\n",
    "pico_text_to_speech(response, output_filename=\"output3.wav\")\n",
    "# Play the response file \"output3.wav\" on device 7\n",
    "play_speech_on_device(file_path=\"output3.wav\", device_index=7)\n",
    "# Example usage: Generate a sound on device 3 for 1 second\n",
    "generate_sound_on_device(device_index=7, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "\n",
    "\n",
    "def convert_audio_params(frames, original_params, target_params):\n",
    "    # Konvertieren der Audiodaten, um sie an die Zielparameter anzupassen\n",
    "    audio_array = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "    # Anpassen der Abtastrate\n",
    "    if original_params.framerate != target_params.framerate:\n",
    "        # Hier könnte eine Bibliothek wie `librosa` oder `scipy` verwendet werden, um die Abtastrate zu ändern\n",
    "        # Resample the audio to the target framerate\n",
    "        num_samples = int(len(audio_array) * target_params.framerate / original_params.framerate)\n",
    "        audio_array = resample(audio_array, num_samples).astype(np.int16)\n",
    "\n",
    "    # Anpassen der Anzahl der Kanäle\n",
    "    if original_params.nchannels != target_params.nchannels:\n",
    "        if target_params.nchannels == 1:\n",
    "            # Mono: Mittelwert der Kanäle\n",
    "            audio_array = np.mean(audio_array.reshape(-1, original_params.nchannels), axis=1).astype(np.int16)\n",
    "        elif target_params.nchannels == 2:\n",
    "            # Stereo: Duplizieren des Monokanals oder Beibehalten der Stereokanäle\n",
    "            if original_params.nchannels == 1:\n",
    "                audio_array = np.repeat(audio_array, 2)\n",
    "\n",
    "    # Anpassen der Sample-Breite (z.B. 16-bit auf 8-bit)\n",
    "    if original_params.sampwidth != target_params.sampwidth:\n",
    "        if target_params.sampwidth == 1:\n",
    "            audio_array = (audio_array >> 8).astype(np.int8)\n",
    "        elif target_params.sampwidth == 2:\n",
    "            audio_array = (audio_array << 8).astype(np.int16)\n",
    "\n",
    "    return audio_array.tobytes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_wav_aggregator(masterfile, addfile):\n",
    "    # Öffnen der Master-Datei, falls vorhanden\n",
    "    if masterfile and os.path.exists(masterfile):\n",
    "        with wave.open(masterfile, 'rb') as master_wav:\n",
    "            master_frames = master_wav.readframes(master_wav.getnframes())\n",
    "            master_params = master_wav.getparams()\n",
    "    else:\n",
    "        master_frames = b''\n",
    "        master_params = None\n",
    "        masterfile = addfile\n",
    "\n",
    "    # Öffnen der hinzuzufügenden Datei\n",
    "    with wave.open(addfile, 'rb') as add_wav:\n",
    "        add_frames = add_wav.readframes(add_wav.getnframes())\n",
    "        add_params = add_wav.getparams()\n",
    "\n",
    "        # Wenn die Master-Datei nicht existiert, verwenden wir die Parameter der hinzuzufügenden Datei\n",
    "        if master_params is None:\n",
    "            master_params = add_params\n",
    "            master_frames = add_frames\n",
    "        else:\n",
    "            # Anpassen der Parameter der hinzuzufügenden Datei an die Master-Datei\n",
    "            if add_params.nchannels != master_params.nchannels or \\\n",
    "               add_params.sampwidth != master_params.sampwidth or \\\n",
    "               add_params.framerate != master_params.framerate:\n",
    "                add_frames = convert_audio_params(add_frames, add_params, master_params)\n",
    "\n",
    "    # Zusammenführen der Audiodaten\n",
    "    master_frames += add_frames\n",
    "\n",
    "    # Schreiben der kombinierten Daten in die Master-Datei\n",
    "    with wave.open(masterfile, 'wb') as combined_wav:\n",
    "        combined_wav.setparams(master_params)\n",
    "        combined_wav.writeframes(master_frames)\n",
    "\n",
    "    return masterfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile=my_wav_aggregator('output1.wav', 'input2.wav')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile=my_wav_aggregator('output1.wav', 'output2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile=my_wav_aggregator('output1.wav', 'input3.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile=my_wav_aggregator('output1.wav', 'output3.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_speech_on_device(file_path=\"output1.wav\", device_index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import wave\n",
    "\n",
    "def list_wav_file_params(file_path):\n",
    "    \"\"\"\n",
    "    List the essential parameters of a WAV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the WAV file.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the parameters of the WAV file.\n",
    "    \"\"\"\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        params = {\n",
    "            'nchannels': wav_file.getnchannels(),\n",
    "            'sampwidth': wav_file.getsampwidth(),\n",
    "            'framerate': wav_file.getframerate(),\n",
    "            'nframes': wav_file.getnframes(),\n",
    "            'comptype': wav_file.getcomptype(),\n",
    "            'compname': wav_file.getcompname()\n",
    "        }\n",
    "    return params\n",
    "\n",
    "# Example usage\n",
    "file_params = list_wav_file_params('output1.wav')\n",
    "print(file_params)\n",
    "file_params = list_wav_file_params('input2.wav')\n",
    "print(file_params)\n",
    "file_params = list_wav_file_params('output2.wav')\n",
    "print(file_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
